#!/usr/bin/env python

"""
Another version of running redshifts per tile

Examples:

All exposures of tile 80605 on night 20201215:

    desi_tile_redshifts --tile 80605 --night 20201215

Exposures E1 E2 E3 on night 20201215 (auto splitting by TILEID if needed):

    desi_tile_redshifts --night 20201215 --expid 67972 67973 67968 67969

Tile 80605 combined across multiple nights:

    desi_tile_redshifts --tile 80605 --night 20201214 20201215 --group blat

Tile 80605 combined across all nights:

    desi_tile_redshifts --tile 80605 --group all

Generate scripts for every tile on 20201215 but don't submit batch jobs:

    desi_tile_redshifts --night 20201215 --nosubmit

Use exposures from a separately curated input list:

    desi_tile_redshifts --explist explist-deep.txt --group deep

Not supported yet: multiple tiles on a single night in a single call:

    desi_tile_redshifts --night 20201215 --tileid 80605 80606 80607

"""

#- Parse command line quickly for --help before slower imports
import argparse
p = argparse.ArgumentParser()
p.add_argument("-n", "--night", type=int, nargs='+', help="YEARMMDD nights")
p.add_argument("-t", "--tileid", type=int, help="Tile ID")
p.add_argument("-e", "--expid", type=int, nargs='+', help="exposure IDs")
p.add_argument("-g", "--group", type=str, help="night group name")
p.add_argument("--explist", type=str,
        help="file with columns TILE NIGHT EXPID to use")
p.add_argument("--nosubmit", action="store_true",
        help="generate scripts but don't submit batch jobs")
p.add_argument("--batch-queue", type=str, default='realtime',
        help="batch queue name")
p.add_argument("--batch-reservation", type=str,
        help="batch reservation name")
p.add_argument("--batch-dependency", type=str,
        help="job dependencies passed to sbatch --dependency")

# TODO
# p.add_argument("--outdir", type=str, help="output directory")
# p.add_argument("--scriptdir", type=str, help="script directory")
# p.add_argument("--per-exposure", action="store_true",
#         help="fit redshifts per exposure instead of grouping")

args = p.parse_args()

import sys, os, glob
import subprocess
import numpy as np
from astropy.table import Table, vstack

from desiutil.log import get_logger

import desispec.io
from desispec.workflow.tableio import load_table
from desispec.workflow.exptable import get_exposure_table_pathname

def batch_tile_redshifts(tileid, exptable, group, spectrographs=None,
    submit=False, queue='realtime', reservation=None, dependency=None):
    """
    Generate batch script for spectra+coadd+redshifts for a tile

    Args:
        tileid (int): Tile ID
        exptable (Table): has columns NIGHT EXPID to use; ignores other columns.
            Doesn't need to be full pipeline exposures table (but could be)
        group (str): group name, e.g. YEARMMDD or "all" or "good"

    Options:
        spectrographs (list of int): spectrographs to include
        submit (bool): also submit batch script to queue
        queue (str): batch queue name
        reservation (str): batch reservation name
        dependency (str): passed to sbatch --dependency upon submit

    Returns tuple (scriptpath, error):
        scriptpath (str): full path to generated script
        err (int): return code from submitting job (0 if submit=False)

    By default this generates the script but don't submit it
    """
    log = get_logger()
    if spectrographs is None:
        spectrographs = (0,1,2,3,4,5,6,7,8,9)

    spectro_string = ' '.join([str(sp) for sp in spectrographs])
    num_nodes = len(spectrographs)

    frame_glob = list()
    for night, expid in zip(exptable['NIGHT'], exptable['EXPID']):
        frame_glob.append(f'exposures/{night}/{expid:08d}/cframe-[brz]$SPECTRO-{expid:08d}.fits')

    frame_glob = ' '.join(frame_glob)

    reduxdir = desispec.io.specprod_root()
    scriptdir = f'{reduxdir}/run/scripts/tiles/{tileid}/{group}'
    os.makedirs(scriptdir, exist_ok=True)
    batchscript = f'{scriptdir}/coadd-redshifts-{tileid}-{group}.slurm'
    batchlog = batchscript.replace('.slurm', r'-%j.log')

    #- TODO: generalize beyond Cori haswell realtime
    with open(batchscript, 'w') as fx:
         fx.write(f"""#!/bin/bash

#SBATCH -C haswell
#SBATCH -N {num_nodes}
#SBATCH --account desi
#SBATCH --qos {queue}
#SBATCH --job-name redrock-{tileid}-{group}
#SBATCH --output {batchlog}
#SBATCH --time=00:15:00
#SBATCH --exclusive

echo Starting at $(date)
    
cd $DESI_SPECTRO_REDUX/$SPECPROD
mkdir -p tiles/{tileid}/{group}
echo Generating files in $(pwd)/tiles/{tileid}/{group}
for SPECTRO in {spectro_string}; do
    spectra=tiles/{tileid}/{group}/spectra-$SPECTRO-{tileid}-{group}.fits
    splog=tiles/{tileid}/{group}/spectra-$SPECTRO-{tileid}-{group}.log

    if [ -f $spectra ]; then
        echo $(basename $spectra) already exists, skipping grouping
    else
        # Check if any input frames exist, while silencing stderr if some
        # of the glob doesn't match (the "2> /dev/null" part)
        CFRAMES=$(ls {frame_glob} 2> /dev/null)
        NUM_CFRAMES=$(echo $CFRAMES | wc -w)
        if [ $NUM_CFRAMES -gt 0 ]; then
            echo Grouping $NUM_CFRAMES cframes into $(basename $spectra)
            srun -N 1 -n 1 -c 64 desi_group_spectra --inframes $CFRAMES --outfile $spectra &> $splog &
            sleep 1
        else
            echo ERROR: no input cframes for spectrograph $SPECTRO, skipping
        fi
    fi
done
echo Waiting for desi_group_spectra to finish at $(date)
wait

for SPECTRO in {spectro_string}; do
    spectra=tiles/{tileid}/{group}/spectra-$SPECTRO-{tileid}-{group}.fits
    coadd=tiles/{tileid}/{group}/coadd-$SPECTRO-{tileid}-{group}.fits
    colog=tiles/{tileid}/{group}/coadd-$SPECTRO-{tileid}-{group}.log

    if [ -f $coadd ]; then
        echo $(basename $coadd) already exists, skipping coadd
    elif [ -f $spectra ]; then
        echo Coadding $(basename $spectra) into $(basename $coadd)
        srun -N 1 -n 1 -c 64 desi_coadd_spectra --nproc 16 -i $spectra -o $coadd &> $colog &
        sleep 1
    else
        echo ERROR: missing $(basename $spectra), skipping coadd
    fi
done
echo Waiting for desi_coadd_spectra to finish at $(date)
wait

for SPECTRO in {spectro_string}; do
    spectra=tiles/{tileid}/{group}/spectra-$SPECTRO-{tileid}-{group}.fits
    zbest=tiles/{tileid}/{group}/zbest-$SPECTRO-{tileid}-{group}.fits
    redrock=tiles/{tileid}/{group}/redrock-$SPECTRO-{tileid}-{group}.h5
    rrlog=tiles/{tileid}/{group}/redrock-$SPECTRO-{tileid}-{group}.log

    if [ -f $zbest ]; then
        echo $(basename $zbest) already exists, skipping redshifts
    elif [ -f $spectra ]; then
        echo Running redrock on $(basename $spectra)
        srun -N 1 -n 32 -c 2 rrdesi_mpi $spectra -o $redrock -z $zbest &> $rrlog &
        sleep 1
    else
        echo ERROR: missing $(basename $spectra), skipping redshifts
    fi
done
echo Waiting for redrock to finish at $(date)
wait
echo Done at $(date)
""")

    log.info(f'Wrote {batchscript}')

    err = 0
    if submit:
        cmd = ['sbatch', batchscript]
        if reservation:
            cmd.extend(['--reservation', reservation])
        if dependency:
            cmd.extend(['--dependency', dependency])

        err = subprocess.call(cmd)
        basename = os.path.basename(batchscript)
        if err == 0:
            log.info(f'submitted {basename}')
        else:
            log.error(f'Error {err} submitting {basename}')

    return batchscript, err


def _read_minimal_exptables(nights=None):
    """
    Read exposure tables while handling evolving formats

    Args:
        nights (list of int): nights to include (default all nights found)

    Returns exptable with just columns TILEID, NIGHT, EXPID filtered by science
        exposures with LASTSTEP='all' and TILEID>=0

    Note: the returned table is *not* the full pipeline exposures table because
        the format of that changed during SV1 and thus can't be stacked without
        trimming down the columns.  This trims to just the minimal columns
        needed by desi_tile_redshifts.
    """
    if nights is None:
        nights = list()
        reduxdir = desispec.io.specprod_root()
        for nightdir in sorted(glob.glob(f'{reduxdir}/exposures/20??????')):
            try:
                n = int(os.path.basename(nightdir))
                nights.append(n)
            except ValueError:
                pass

    exptables = list()
    for night in nights:
        t = Table.read(get_exposure_table_pathname(night))
        keep = (t['OBSTYPE'] == 'science') & (t['TILEID'] >= 0)
        if 'LASTSTEP' in t.colnames:        
            keep &= (t['LASTSTEP'] == 'all')

        t = t[keep]
        exptables.append(t['TILEID', 'NIGHT', 'EXPID'])

    return vstack(exptables)

#-------------------------------------------------------------------------
log = get_logger()

#- If --tileid, --night, and --expid are all given, create exptable
if ((args.tileid is not None) and
    (args.night is not None) and (len(args.night) == 1) and
    (args.expid is not None)):
    log.info('Creating exposure table from --tileid --night --expid options')
    exptable = Table()
    exptable['EXPID'] = args.expid
    exptable['NIGHT'] = args.night[0]
    exptable['TILEID'] = args.tileid

    if args.explist is not None:
        log.warning('Ignoring --explist, using --tileid --night --expid')

#- otherwise load exposure tables for those nights
elif args.explist is None:
    if args.night is not None:
        log.info(f'Loading production exposure tables for nights {args.night}')
    else:
        log.info(f'Loading production exposure tables for all nights')

    exptable = _read_minimal_exptables(args.night)

else:
    log.info(f'Loading exposure list from {args.explist}')
    if args.explist.endswith('.fits'):
        exptable = Table.read(args.explist, format='fits')
    elif args.explist.endswith('.csv'):
        exptable = Table.read(args.explist, format='ascii.csv')
    elif args.explist.endswith('.ecsv'):
        exptable = Table.read(args.explist, format='ascii.ecsv')
    else:
        exptable = Table.read(args.explist, format='ascii')

    if args.night is not None:
        keep = np.in1d(exptable['NIGHT'], args.night)
        exptable = exptable[keep]

#- Filter exposure tables by exposure IDs or by tileid
#- Note: If exptable was created from --expid --night --tileid these should
#- have no effect, but are left in for code flow simplicity
if args.expid is not None:
    keep = np.in1d(exptable['EXPID'], args.expid)
    exptable = exptable[keep]
    expids = np.array(exptable['EXPID'])
    tileids = np.unique(np.array(exptable['TILEID']))

    #- if provided, tileid should be redundant with the tiles in those exps
    if args.tileid is not None:
        if not np.all(exptable['TILEID'] == args.tileid):
            log.critical(f'Exposure TILEIDs={tileids} != --tileid={args.tileid}')
            sys.exit(1)
 
elif args.tileid is not None:
    keep = (exptable['TILEID'] == args.tileid)
    exptable = exptable[keep]
    expids = np.array(exptable['EXPID'])
    tileids = np.array([args.tileid,])

else:
    tileids = np.unique(np.array(exptable['TILEID']))

#- anything left?
if len(exptable) == 0:
    log.critical(f'No exposures left after filtering by tileid/night/expid')
    sys.exit(1)

#- default group is the YEARMMDD night, but must be given if combining nights
if args.group is not None:
    group = args.group
else:
    if len(np.unique(exptable['NIGHT'])) == 1:
        group = str(exptable['NIGHT'][0])
    else:
        log.critical('If using more than one night, must specify --group')
        sys.exit(1)

#- Generate the scripts and optionally submit them
failed_jobs = list()
for tileid in tileids:
    tilerows = exptable['TILEID'] == tileid
    nights = np.unique(np.array(exptable['NIGHT'][tilerows]))
    expids = np.unique(np.array(exptable['EXPID'][tilerows]))
    log.info(f'Tile {tileid} nights={nights} expids={expids}')
    submit = (not args.nosubmit)
    batchscript, batcherr = batch_tile_redshifts(
            tileid, exptable[tilerows], group, submit=submit,
            queue=args.batch_queue, reservation=args.batch_reservation,
            dependency=args.batch_dependency
            )

    if batcherr != 0:
        failed_jobs.append(batchscript)

num_error = len(failed_jobs)
if num_error > 0:
    tmp = [os.path.basename(filename) for filename in failed_jobs]
    log.error(f'problem submitting {num_error} scripts: {tmp}')

sys.exit(num_error)



