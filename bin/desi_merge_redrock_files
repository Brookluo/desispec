#!/usr/bin/env python

"""
Make summary redshift catalogs from tile-based redshifts, e.g. blanc
"""

import os, sys, glob, collections
import argparse
import numpy as np
from numpy.lib.recfunctions import rec_append_fields, join_by
import fitsio
from astropy.table import Table, vstack
from desiutil.log import get_logger
from desiutil import depend

parser = argparse.ArgumentParser(usage = "{prog} [options]")
parser.add_argument("--group", type=str, required=True,
        help="exposure grouping, e.g. 'all' or 'deep'")
parser.add_argument("-o", "--output", type=str, required=True,
        help="output catalog filename")
parser.add_argument("--reduxdir", type=str,
        help="overrides $DESI_SPECTRO_REDUX/$SPECPROD")
parser.add_argument("--first-tile", type=int,
        help="First TILEID to include")
parser.add_argument("--last-tile", type=int,
        help="Last TILEID to include (inclusive, not python indexing style)")
parser.add_argument("--tiles", type=int, nargs="*",
        help="TILEIDs to include (space separated)")
parser.add_argument("--no-fibermap", action="store_true", help="Do not merge with fibermap")
parser.add_argument("--no-scores", action="store_true", help="Do not merge with coadd scores")
# parser.add_argument("-v", "--verbose", action="store_true", help="some flag")

args = parser.parse_args()
log = get_logger()

if args.reduxdir is None:
    args.reduxdir = os.path.expandvars('$DESI_SPECTRO_REDUX/$SPECPROD')

tiledir = f'{args.reduxdir}/tiles/{args.group}'
log.info(f'Looking for per-tile redrock files in {tiledir}/TILEID')
assert os.path.isdir(tiledir)

rrfiles = list()
for filename in sorted(glob.glob(f'{tiledir}/*/*/redrock-*.fits')):
    tileid = int(os.path.basename(os.path.dirname(os.path.dirname(filename))))
    if args.first_tile is not None and tileid < args.first_tile:
        continue
    if args.last_tile is not None and tileid > args.last_tile:
        continue
    if args.tiles is not None and tileid not in args.tiles:
        continue

    rrfiles.append(filename)

if len(rrfiles) == 0:
    log.critical('No redrock files found; exiting')
    sys.exit(1)

nrrfiles = len(rrfiles)
rrx = list()

for i, filename in enumerate(rrfiles):
    print(f'{i+1}/{nrrfiles}: {filename}')
    rr = fitsio.read(filename, 'REDSHIFTS')

    tmp = os.path.basename(filename).replace('redrock-', 'coadd-', 1)
    coaddfile = os.path.join(os.path.dirname(filename), tmp)
    if not args.no_fibermap:
        fm = fitsio.read(filename, 'FIBERMAP')
        #- Sorted the same
        assert np.all(rr['TARGETID'] == fm['TARGETID'])
        #- TARGETID is only column in common
        assert (set(rr.dtype.names) & set(fm.dtype.names)) == set(['TARGETID',])
        rr = join_by('TARGETID', rr, fm)
    else:
        tileid = np.ones(len(rr), dtype=np.int16)*tileid
        rr = rec_append_fields(rr, 'TILEID', tileid)

    if not args.no_scores:
        scores = fitsio.read(coaddfile, 'SCORES')

        #- Same targets
        assert len(rr) == len(scores)
        assert np.all(np.isin(scores['TARGETID'], rr['TARGETID']))

        #- TARGETID is only column in common
        assert (set(rr.dtype.names) & set(scores.dtype.names)) == set(['TARGETID',])
        rr = join_by('TARGETID', rr, scores)

    #- Handle a few dtype special cases
    rr = Table(rr)
    if 'NUMOBS_MORE' in rr.colnames and rr['NUMOBS_MORE'].dtype != np.dtype('>i4'):
        rr['NUMOBS_MORE'] = rr['NUMOBS_MORE'].astype('>i4')
    if 'RELEASE' in rr.colnames and rr['RELEASE'].dtype != np.dtype('>i2'):
        rr['RELEASE'] = rr['RELEASE'].astype('>i2')

    rrx.append(rr)

#- Determine union and intersection of columns present in the files
#- since the fiberassign datamodel evolved for exactly which targeting
#- columns were provided
all_columns = set()
joint_columns = None
for rr in rrx:
    all_columns.update(rr.colnames)
    if joint_columns is None:
        joint_columns = set(rr.colnames)
    else:
        joint_columns &= set(rr.colnames)

#- Add *_TARGET columns if needed, and drop other columns that aren't in common
add_columns = set()
drop_columns = set()
for colname in (all_columns - joint_columns):
    if colname.endswith('_TARGET'):
        add_columns.add(colname)
    else:
        drop_columns.add(colname)

#- update individual tables to have the same columns
for rr in rrx:
    for colname in add_columns:
        if colname not in rr.colnames:
            rr[colname] = np.zeros(len(rr), dtype=int)
    for colname in drop_columns:
        if colname in list(rr.colnames):
            rr.remove_column(colname)

#- Standardize column order to match last tile
columns = rrx[-1].colnames
for i in range(len(rrx)):
    rr = rrx[i]
    if rr.columns != columns:
        rrx[i] = rr[columns]  #- reorders columns

#- Finally! make the stacks redshift catalog
zcat = vstack(rrx)

#- Add record of inputs
zcat.meta['TILEDIR'] = os.path.normpath(tiledir)
for i, filename in enumerate(rrfiles):
    key = f'IN{i:06d}'
    zcat.meta[key] = filename.replace(tiledir, 'TILEDIR')

depend.add_dependencies(zcat.meta)
zcat.meta['SPECPROD'] = os.path.basename(os.path.normpath(args.reduxdir))
zcat.meta['EXTNAME'] = 'ZCATALOG'
tmpname = args.output+'.tmp'
zcat.write(tmpname, format='fits')
os.rename(tmpname, args.output)


