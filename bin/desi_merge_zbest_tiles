#!/usr/bin/env python

"""
Make summary redshift catalogs from tile-based redshifts, e.g. blanc
"""

import os, sys, glob, collections
import argparse
import numpy as np
from numpy.lib.recfunctions import rec_append_fields, join_by
import fitsio
from astropy.table import Table, vstack
from desiutil.log import get_logger
from desiutil import depend

parser = argparse.ArgumentParser(usage = "{prog} [options]")
parser.add_argument("--group", type=str, required=True,
        help="exposure grouping, e.g. 'all' or 'deep'")
parser.add_argument("-o", "--output", type=str, required=True,
        help="output catalog filename")
parser.add_argument("--reduxdir", type=str,
        help="overrides $DESI_SPECTRO_REDUX/$SPECPROD")
parser.add_argument("--first-tile", type=int,
        help="First TILEID to include")
parser.add_argument("--last-tile", type=int,
        help="Last TILEID to include (inclusive, not python indexing style)")
parser.add_argument("--tiles", type=int, nargs="*",
        help="TILEIDs to include (space separated)")
parser.add_argument("--no-fibermap", action="store_true", help="Do not merge with fibermap")
parser.add_argument("--no-scores", action="store_true", help="Do not merge with coadd scores")
# parser.add_argument("-v", "--verbose", action="store_true", help="some flag")

args = parser.parse_args()
log = get_logger()

if args.reduxdir is None:
    args.reduxdir = os.path.expandvars('$DESI_SPECTRO_REDUX/$SPECPROD')

tiledir = f'{args.reduxdir}/tiles'
log.info(f'Looking for per-tile zbest files in {tiledir}/TILEID/{args.group}/')
assert os.path.isdir(tiledir)

zbestfiles = list()
for filename in sorted(glob.glob(f'{tiledir}/*/{args.group}/zbest-*.fits')):
    tileid = int(os.path.basename(os.path.dirname(os.path.dirname(filename))))
    if args.first_tile is not None and tileid < args.first_tile:
        continue
    if args.last_tile is not None and tileid > args.last_tile:
        continue
    if args.tiles is not None and tileid not in args.tiles:
        continue

    zbestfiles.append(filename)

nzbfiles = len(zbestfiles)
zbx = list()

for i, filename in enumerate(zbestfiles):
    print(f'{i+1}/{nzbfiles}: {filename}')
    zb = fitsio.read(filename, 'ZBEST')

    tmp = os.path.basename(filename).replace('zbest-', 'coadd-', 1)
    coaddfile = os.path.join(os.path.dirname(filename), tmp)
    if not args.no_fibermap:
        fm = fitsio.read(coaddfile, 'FIBERMAP')
        #- Sorted the same
        assert np.all(zb['TARGETID'] == fm['TARGETID'])
        #- TARGETID is only column in common
        assert (set(zb.dtype.names) & set(fm.dtype.names)) == set(['TARGETID',])
        zb = join_by('TARGETID', zb, fm)
    else:
        tileid = np.ones(len(zb), dtype=np.int16)*tileid
        zb = rec_append_fields(zb, 'TILEID', tileid)

    if not args.no_scores:
        scores = fitsio.read(coaddfile, 'SCORES')
        #- Sorted the same
        assert np.all(zb['TARGETID'] == scores['TARGETID'])
        #- TARGETID is only column in common
        assert (set(zb.dtype.names) & set(scores.dtype.names)) == set(['TARGETID',])
        zb = join_by('TARGETID', zb, scores)

    #- Handle a few dtype special cases
    zb = Table(zb)
    if 'NUMOBS_MORE' in zb.colnames and zb['NUMOBS_MORE'].dtype != np.dtype('>i4'):
        zb['NUMOBS_MORE'] = zb['NUMOBS_MORE'].astype('>i4')
    if 'RELEASE' in zb.colnames and zb['RELEASE'].dtype != np.dtype('>i2'):
        zb['RELEASE'] = zb['RELEASE'].astype('>i2')

    zbx.append(zb)

#- Determine union and intersection of columns present in the files
#- since the fiberassign datamodel evolved for exactly which targeting
#- columns were provided
all_columns = set()
joint_columns = None
for zb in zbx:
    all_columns.update(zb.colnames)
    if joint_columns is None:
        joint_columns = set(zb.colnames)
    else:
        joint_columns &= set(zb.colnames)

#- Add *_TARGET columns if needed, and drop other columns that aren't in common
add_columns = set()
drop_columns = set()
for colname in (all_columns - joint_columns):
    if colname.endswith('_TARGET'):
        add_columns.add(colname)
    else:
        drop_columns.add(colname)

#- update individual tables to have the same columns
for zb in zbx:
    for colname in add_columns:
        if colname not in zb.colnames:
            zb[colname] = np.zeros(len(zb), dtype=int)
    for colname in drop_columns:
        if colname in list(zb.colnames):
            zb.remove_column(colname)

#- Standardize column order to match last tile
columns = zbx[-1].colnames
for i in range(len(zbx)):
    zb = zbx[i]
    if zb.columns != columns:
        zbx[i] = zb[columns]  #- reorders columns

#- Finally! make the stacks redshift catalog
zcat = vstack(zbx)

#- Add record of inputs
zcat.meta['TILEDIR'] = os.path.normpath(tiledir)
for i, filename in enumerate(zbestfiles):
    key = f'IN{i:06d}'
    zcat.meta[key] = filename.replace(tiledir, 'TILEDIR')

depend.add_dependencies(zcat.meta)
zcat.meta['SPECPROD'] = os.path.basename(os.path.normpath(args.reduxdir))
zcat.meta['EXTNAME'] = 'ZCATALOG'
zcat.write(args.output)


